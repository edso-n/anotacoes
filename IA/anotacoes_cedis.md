## ğŸ“¦ ConfiguraÃ§Ã£o de Containers

### ğŸ§  Sobre

Trata-se de uma arquitetura multimodal utilizando LLMs.
Atualmente sÃ£o usadas trÃªs ferramentas principais:

* **Ollama**
* **OpenWebUI**
* **LLMlite**

#### ğŸ”— IntegraÃ§Ã£o entre os serviÃ§os

* **OpenWebUI**
  Interface grÃ¡fica do usuÃ¡rio.
  Conectada ao **LLMlite** pela rede local na porta **4000**.

* **LLMlite**
  Atua como maestro/gerenciador responsÃ¡vel por decidir qual modelo de IA serÃ¡ utilizado.
  Conecta-se aos containers do **Ollama** em diferentes portas.

* **Ollama**
  ResponsÃ¡vel pelo download e execuÃ§Ã£o dos modelos de IA.

---

### ğŸ³ Criando Containers

#### OpenWebUI

```bash
docker run --detach --interactive --tty --name ctrOpenweb --workdir /openweb --network host andrelanna/openweb-ui:0.6.4 bash
```
#### Ollama

```bash
docker run --detach --interactive --tty --name ctrOllama --workdir /ollama --network host andrelanna/ollama bash
```
#### LLMlite

```bash
docker run -d \
    --name ctrLLMlite \
    --network host \
    -v $(pwd)/litellm_config.yaml:/app/config.yaml \
    docker.litellm.ai/berriai/litellm:main-latest \
    --config /app/config.yaml
```    

### ğŸ³ Rodando Containers

#### OpenWebUI

```bash
docker start ctrOpenweb 
docker exec --interactive --tty --name ctrOpenweb bash
open-webui serve
```
#### Ollama

```bash
docker start ctrOllama 
docker exec  --interactive --tty --name ctrOllama bash
ollama serve
OLLAMA_HOST=127.0.0.1:11435 ollama serve #INICIA UM SEGUNDO CONTAINER NA PORTA 11435
```
#### LLMlite

```bash
docker start ctrLLMlite
```  

Para ver os logs (Qual modelo estÃ¡ te respondendo)

```bash
docker logs -f ctrLLMlite 2>&1 | grep -E "model|downstream"
```


# CAMINHOS
---

### NÃ­vel 1: O "Fallback" (AutomaÃ§Ã£o de Falha)

**CenÃ¡rio:** VocÃª quer usar o **Llama 3** sempre. Mas se ele estiver travado ou o container cair, o sistema deve mudar automaticamente para o **DeepSeek** sem te dar erro.

**Como fazer no YAML:**
VocÃª cria um modelo principal e define uma lista de "backups".

```yaml
model_list:
  - model_name: gpt-automatico #nome do modelo
    litellm_params:
      model: ollama/llama3
      api_base: http://localhost:11434
      fallback_models: ["backup-model"] # Se falhar, chama este

  - model_name: backup-model # O modelo de seguranÃ§a
    litellm_params:
      model: ollama/mistral
      api_base: http://localhost:11435 #em outro container

```

---

### NÃ­vel 2: Balanceamento de Carga (AutomaÃ§Ã£o de Velocidade)

**CenÃ¡rio:** 2 containers Ollama rodando o mesmo modelo. O LiteLLM manda o prompt para aquele que estiver **menos ocupado** ou responda **mais rÃ¡pido**.

**Como fazer no YAML:**
VocÃª usa o mesmo `model_name` para vÃ¡rias entradas. O LiteLLM entende que Ã© um grupo.

```yaml
model_list:
  - model_name: gpt-turbo # Nome igual
    litellm_params:
      model: ollama/llama3
      api_base: http://localhost:11434 # Servidor 1

  - model_name: gpt-turbo # Nome igual
    litellm_params:
      model: ollama/llama3
      api_base: http://localhost:11435 # Servidor 2

router_settings:
  routing_strategy: usage-based-routing-v2 # Manda para quem estÃ¡ trabalhando menos
  # ou use: latency-based-routing (Manda para quem responde mais rÃ¡pido)

```

---

### NÃ­vel 3: Roteamento SemÃ¢ntico (O "Santo Graal")

**CenÃ¡rio:** Ã‰ o que vocÃª perguntou.

* Prompt: *"Crie uma funÃ§Ã£o em Python..."* -> **LiteLLM detecta e manda para o DeepSeek Coder.**
* Prompt: *"Resuma este texto..."* -> **LiteLLM detecta e manda para o Llama 3.**

**A Verdade:**
Fazer isso **apenas pelo YAML** Ã© limitado. O LiteLLM Proxy suporta roteamento bÃ¡sico por regex (palavras-chave), mas para uma inteligÃªncia real, geralmente usamos uma biblioteca auxiliar chamada `semantic-router` (que tambÃ©m Ã© open-source e integra com o LiteLLM).

**Mas... dÃ¡ para fazer um "truque" com Regex no YAML!**
VocÃª pode tentar configurar rotas baseadas no conteÃºdo do prompt, embora seja um recurso mais avanÃ§ado e experimental no config.

Exemplo conceitual de como seria a lÃ³gica (via Python SDK seria mais robusto, mas via config tentamos assim):

```yaml
router_settings:
  model_group_alias:
    "gpt-code": "ollama/deepseek-coder"
    "gpt-chat": "ollama/llama3"

```

No entanto, a maneira mais sÃ³lida de fazer isso num projeto de Engenharia de Software Ã© criar um pequeno **script Python** que usa a biblioteca `litellm` em vez de usar apenas o binÃ¡rio do Proxy.

### O "Pulo do Gato" para o seu Projeto (Open WebUI Functions)

Como vocÃª jÃ¡ usa o **Open WebUI**, existe um jeito mais fÃ¡cil de fazer essa mÃ¡gica sem mexer no LiteLLM!

O Open WebUI tem um recurso chamado **Filters (Functions)**.

1. No Open WebUI, vÃ¡ em **Workspace > Functions**.
2. VocÃª pode criar um filtro em Python que analisa o prompt antes de enviar.
3. Se o prompt contiver palavras como `def`, `class`, `python`, `code`, vocÃª forÃ§a a troca do modelo (model override).

**VocÃª quer que eu gere esse script Python para vocÃª colocar dentro do Open WebUI e fazer essa troca automÃ¡tica?** (Ã‰ a soluÃ§Ã£o mais elegante para o seu stack atual). 