## ğŸ“¦ ConfiguraÃ§Ã£o de Containers

### ğŸ§  Sobre

Trata-se de uma arquitetura multimodal utilizando LLMs.
Atualmente sÃ£o usadas trÃªs ferramentas principais:

* **Ollama**
* **OpenWebUI**
* **LLMlite**

#### ğŸ”— IntegraÃ§Ã£o entre os serviÃ§os

* **OpenWebUI**
  Interface grÃ¡fica do usuÃ¡rio.
  Conectada ao **LLMlite** pela rede local na porta **4000**.

* **LLMlite**
  Atua como maestro/gerenciador responsÃ¡vel por decidir qual modelo de IA serÃ¡ utilizado.
  Conecta-se aos containers do **Ollama** em diferentes portas.

* **Ollama**
  ResponsÃ¡vel pelo download e execuÃ§Ã£o dos modelos de IA.

---

### ğŸ³ Criando Containers

#### OpenWebUI

```bash
docker run --detach --interactive --tty --name ctrOpenweb --workdir /openweb --network host andrelanna/openweb-ui:0.6.4 bash
```
### Ollama

```bash
docker run --detach --interactive --tty --name ctrOllama --workdir /ollama --network host andrelanna/ollama bash
```

### ğŸ³ Rodando Containers

#### OpenWebUI

```bash
docker start ctrOpenweb 
docker exec --interactive --tty --name ctrOpenweb bash
open-webui serve
```
### Ollama

```bash
docker start ctrOllama 
docker exec  --interactive --tty --name ctrOllama bash
ollama serve
OLLAMA_HOST=127.0.0.1:11435 ollama serve #INICIA UM SEGUNDO CONTAINER NA PORTA 11435
```
